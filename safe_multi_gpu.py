#!/usr/bin/env python3
"""
ì•ˆì „í•œ ë©€í‹° GPU êµ¬í˜„
DataParallel ëŒ€ì‹  ìˆ˜ë™ ë°°ì¹˜ ë¶„í• ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
from typing import List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class SafeMultiGPUWrapper(nn.Module):
    """ì•ˆì „í•œ ë©€í‹° GPU ë˜í¼ - DataParallel ëŒ€ì‹  ìˆ˜ë™ ë°°ì¹˜ ë¶„í• """
    
    def __init__(self, model: nn.Module, device_ids: List[int]):
        super().__init__()
        self.model = model
        self.device_ids = device_ids
        self.main_device = device_ids[0]
        
        # ê° GPUì— ëª¨ë¸ ë³µì‚¬ë³¸ ìƒì„±
        self.replicas = []
        for device_id in device_ids:
            device = torch.device(f'cuda:{device_id}')
            replica = self._create_replica(model, device)
            self.replicas.append(replica)
        
        logger.info(f"ğŸš€ SafeMultiGPUWrapper ì´ˆê¸°í™” ì™„ë£Œ: {len(device_ids)}ê°œ GPU")
    
    def _create_replica(self, model: nn.Module, device: torch.device) -> nn.Module:
        """ëª¨ë¸ ë³µì‚¬ë³¸ ìƒì„±"""
        # ëª¨ë¸ì„ í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        replica = model.to(device)
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸
        for param in replica.parameters():
            if param.device != device:
                param.data = param.data.to(device)
        
        return replica
    
    def forward(self, *inputs, **kwargs):
        """ìˆœì „íŒŒ - ë°°ì¹˜ë¥¼ GPUë³„ë¡œ ë¶„í• """
        if len(inputs) == 0:
            return None
        
        # ì²« ë²ˆì§¸ ì…ë ¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° í™•ì¸
        batch_size = inputs[0].size(0)
        
        if batch_size < len(self.device_ids):
            # ë°°ì¹˜ í¬ê¸°ê°€ GPU ìˆ˜ë³´ë‹¤ ì‘ìœ¼ë©´ ë‹¨ì¼ GPU ì‚¬ìš©
            main_device = torch.device(f'cuda:{self.main_device}')
            inputs_on_main = tuple(inp.to(main_device) if torch.is_tensor(inp) else inp for inp in inputs)
            kwargs_on_main = {k: v.to(main_device) if torch.is_tensor(v) else v for k, v in kwargs.items()}
            return self.replicas[0](*inputs_on_main, **kwargs_on_main)
        
        # ë°°ì¹˜ë¥¼ GPUë³„ë¡œ ë¶„í• 
        chunk_size = batch_size // len(self.device_ids)
        remainder = batch_size % len(self.device_ids)
        
        input_chunks = []
        kwargs_chunks = []
        
        start_idx = 0
        for i, device_id in enumerate(self.device_ids):
            # ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ GPUë“¤ì— 1ê°œì”© ë” í• ë‹¹
            current_chunk_size = chunk_size + (1 if i < remainder else 0)
            end_idx = start_idx + current_chunk_size
            
            device = torch.device(f'cuda:{device_id}')
            
            # ì…ë ¥ì„ ì²­í¬ë¡œ ë¶„í• í•˜ê³  í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            input_chunk = tuple(
                inp[start_idx:end_idx].to(device) if torch.is_tensor(inp) else inp 
                for inp in inputs
            )
            
            kwargs_chunk = {}
            for k, v in kwargs.items():
                if torch.is_tensor(v) and v.size(0) == batch_size:
                    kwargs_chunk[k] = v[start_idx:end_idx].to(device)
                else:
                    kwargs_chunk[k] = v
            
            input_chunks.append(input_chunk)
            kwargs_chunks.append(kwargs_chunk)
            start_idx = end_idx
        
        # ê° GPUì—ì„œ ìˆœì „íŒŒ ìˆ˜í–‰
        outputs = []
        for i, (replica, input_chunk, kwargs_chunk) in enumerate(zip(self.replicas, input_chunks, kwargs_chunks)):
            try:
                output = replica(*input_chunk, **kwargs_chunk)
                outputs.append(output)
            except Exception as e:
                logger.error(f"âŒ GPU {self.device_ids[i]}ì—ì„œ ìˆœì „íŒŒ ì‹¤íŒ¨: {e}")
                raise
        
        # ì¶œë ¥ ê²°í•©
        return self._combine_outputs(outputs)
    
    def _combine_outputs(self, outputs: List[Any]) -> Any:
        """GPUë³„ ì¶œë ¥ì„ ê²°í•©"""
        if not outputs:
            return None
        
        # ì²« ë²ˆì§¸ ì¶œë ¥ì˜ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
        first_output = outputs[0]
        
        if isinstance(first_output, torch.Tensor):
            # í…ì„œì¸ ê²½ìš° concatenate
            main_device = torch.device(f'cuda:{self.main_device}')
            outputs_on_main = [out.to(main_device) for out in outputs]
            return torch.cat(outputs_on_main, dim=0)
        
        elif isinstance(first_output, dict):
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° í‚¤ë³„ë¡œ concatenate
            combined = {}
            main_device = torch.device(f'cuda:{self.main_device}')
            
            for key in first_output.keys():
                if torch.is_tensor(first_output[key]):
                    tensors = [out[key].to(main_device) for out in outputs]
                    combined[key] = torch.cat(tensors, dim=0)
                else:
                    # í…ì„œê°€ ì•„ë‹Œ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                    combined[key] = first_output[key]
            
            return combined
        
        elif isinstance(first_output, (list, tuple)):
            # ë¦¬ìŠ¤íŠ¸/íŠœí”Œì¸ ê²½ìš° elementë³„ë¡œ ì²˜ë¦¬
            combined = []
            main_device = torch.device(f'cuda:{self.main_device}')
            
            for i in range(len(first_output)):
                if torch.is_tensor(first_output[i]):
                    tensors = [out[i].to(main_device) for out in outputs]
                    combined.append(torch.cat(tensors, dim=0))
                else:
                    combined.append(first_output[i])
            
            return type(first_output)(combined)
        
        else:
            # ê¸°íƒ€ íƒ€ì…ì€ ì²« ë²ˆì§¸ ê°’ ë°˜í™˜
            return first_output

def create_safe_multi_gpu_model(model: nn.Module, device_ids: List[int] = None) -> nn.Module:
    """ì•ˆì „í•œ ë©€í‹° GPU ëª¨ë¸ ìƒì„±"""
    if device_ids is None:
        device_ids = list(range(torch.cuda.device_count()))
    
    if len(device_ids) <= 1:
        # ë‹¨ì¼ GPU
        device = torch.device(f'cuda:{device_ids[0]}' if device_ids else 'cuda:0')
        return model.to(device)
    
    # ë©€í‹° GPU
    return SafeMultiGPUWrapper(model, device_ids)
