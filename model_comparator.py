#!/usr/bin/env python3
"""
ëª¨ë¸ ë¹„êµ ë° ì¶”ê°€ í•™ìŠµ ìœ í‹¸ë¦¬í‹°
"""

import torch
import logging
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from advanced_config import AdvancedTrainingConfig, TrainingStageConfig
from advanced_trainer import AdvancedSignLanguageTrainer
from sign_language_model import SequenceToSequenceSignModel

logger = logging.getLogger(__name__)

class ModelComparator:
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„"""
    
    def __init__(self, checkpoint_dir: str = "./advanced_checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        
    def load_model_info(self, model_path: Path) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë¡œë“œ"""
        try:
            checkpoint = torch.load(model_path, map_location='cpu')
            return {
                'path': str(model_path),
                'stage': checkpoint.get('stage', 'unknown'),
                'epoch': checkpoint.get('epoch', -1),
                'val_loss': checkpoint.get('val_loss', float('inf')),
                'val_accuracy': checkpoint.get('val_accuracy', 0.0),
                'config': checkpoint.get('config', {}),
                'model_size': sum(p.numel() for p in checkpoint['model_state_dict'].values())
            }
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ {model_path}: {e}")
            return None
    
    def compare_models(self, experiment_name: Optional[str] = None) -> pd.DataFrame:
        """ì €ì¥ëœ ëª¨ë¸ë“¤ ë¹„êµ"""
        if experiment_name:
            model_pattern = f"*{experiment_name}*/best_model_stage_*.pt"
        else:
            model_pattern = "best_model_stage_*.pt"
        
        model_files = list(self.checkpoint_dir.glob(model_pattern))
        if not model_files:
            model_files = list(self.checkpoint_dir.rglob("best_model_stage_*.pt"))
        
        logger.info(f"ë°œê²¬ëœ ëª¨ë¸: {len(model_files)}ê°œ")
        
        model_infos = []
        for model_path in sorted(model_files):
            info = self.load_model_info(model_path)
            if info:
                model_infos.append(info)
        
        if not model_infos:
            logger.warning("ë¹„êµí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(model_infos)
        
        # ì •ë ¬ (ê²€ì¦ ì •í™•ë„ ê¸°ì¤€)
        df = df.sort_values('val_accuracy', ascending=False)
        
        return df
    
    def print_comparison(self, df: pd.DataFrame):
        """ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        if df.empty:
            print("ë¹„êµí•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        
        for idx, row in df.iterrows():
            print(f"\nğŸ† ìˆœìœ„ {idx+1}: {Path(row['path']).name}")
            print(f"  ë‹¨ê³„: {row['stage']}")
            print(f"  ê²€ì¦ ì†ì‹¤: {row['val_loss']:.4f}")
            print(f"  ê²€ì¦ ì •í™•ë„: {row['val_accuracy']:.3f}")
            print(f"  ëª¨ë¸ í¬ê¸°: {row['model_size']:,} íŒŒë¼ë¯¸í„°")
            if isinstance(row['config'], dict) and 'label_smoothing' in row['config']:
                print(f"  ë¼ë²¨ ìŠ¤ë¬´ë”©: {row['config']['label_smoothing']}")
                print(f"  ë“œë¡­ì•„ì›ƒ: {row['config'].get('dropout_rate', 'N/A')}")
                print(f"  ì¦ê°• í™œì„±í™”: {row['config'].get('enable_augmentation', False)}")
        
        print("\n" + "="*80)
        print(f"ğŸ¥‡ ìµœê³  ì„±ëŠ¥: {df.iloc[0]['stage']} (ì •í™•ë„: {df.iloc[0]['val_accuracy']:.3f})")
        print("="*80)
    
    def plot_comparison(self, df: pd.DataFrame, save_path: Optional[str] = None):
        """ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™”"""
        if df.empty or len(df) < 2:
            logger.warning("ì‹œê°í™”í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16)
        
        # 1. ê²€ì¦ ì •í™•ë„ ë¹„êµ
        axes[0,0].bar(range(len(df)), df['val_accuracy'])
        axes[0,0].set_title('ê²€ì¦ ì •í™•ë„')
        axes[0,0].set_xlabel('ëª¨ë¸ ìˆœìœ„')
        axes[0,0].set_ylabel('ì •í™•ë„')
        axes[0,0].set_xticks(range(len(df)))
        axes[0,0].set_xticklabels([f"Stage {i+1}" for i in range(len(df))])
        
        # 2. ê²€ì¦ ì†ì‹¤ ë¹„êµ
        axes[0,1].bar(range(len(df)), df['val_loss'])
        axes[0,1].set_title('ê²€ì¦ ì†ì‹¤')
        axes[0,1].set_xlabel('ëª¨ë¸ ìˆœìœ„')
        axes[0,1].set_ylabel('ì†ì‹¤')
        axes[0,1].set_xticks(range(len(df)))
        axes[0,1].set_xticklabels([f"Stage {i+1}" for i in range(len(df))])
        
        # 3. ì •í™•ë„ vs ì†ì‹¤
        axes[1,0].scatter(df['val_loss'], df['val_accuracy'])
        axes[1,0].set_title('ì •í™•ë„ vs ì†ì‹¤')
        axes[1,0].set_xlabel('ê²€ì¦ ì†ì‹¤')
        axes[1,0].set_ylabel('ê²€ì¦ ì •í™•ë„')
        
        # 4. ëª¨ë¸ í¬ê¸° ë¹„êµ
        axes[1,1].bar(range(len(df)), df['model_size'] / 1e6)  # ë°±ë§Œ íŒŒë¼ë¯¸í„° ë‹¨ìœ„
        axes[1,1].set_title('ëª¨ë¸ í¬ê¸° (M íŒŒë¼ë¯¸í„°)')
        axes[1,1].set_xlabel('ëª¨ë¸ ìˆœìœ„')
        axes[1,1].set_ylabel('íŒŒë¼ë¯¸í„° ìˆ˜ (ë°±ë§Œ)')
        axes[1,1].set_xticks(range(len(df)))
        axes[1,1].set_xticklabels([f"Stage {i+1}" for i in range(len(df))])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ì°¨íŠ¸ ì €ì¥: {save_path}")
        
        plt.show()

class ContinualLearningManager:
    """ì§€ì†ì  í•™ìŠµ ê´€ë¦¬ì"""
    
    def __init__(self, base_config: AdvancedTrainingConfig):
        self.base_config = base_config
        
    def create_transfer_config(self, 
                             source_model_path: str,
                             target_stages: List[TrainingStageConfig],
                             experiment_suffix: str = "transfer") -> AdvancedTrainingConfig:
        """ì „ì´ í•™ìŠµ ì„¤ì • ìƒì„±"""
        config = AdvancedTrainingConfig()
        
        # ê¸°ë³¸ ì„¤ì • ë³µì‚¬
        config.random_seed = self.base_config.random_seed
        config.data_split = self.base_config.data_split
        config.annotation_path = self.base_config.annotation_path
        config.pose_data_dir = self.base_config.pose_data_dir
        
        # ìƒˆ ì‹¤í—˜ ì´ë¦„
        config.experiment_name = f"{self.base_config.experiment_name}_{experiment_suffix}"
        
        # ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê·¸ ë””ë ‰í† ë¦¬
        config.checkpoint_dir = f"./advanced_checkpoints/{config.experiment_name}"
        config.log_dir = f"./advanced_logs/{config.experiment_name}"
        
        # ì „ì´ í•™ìŠµ ë‹¨ê³„ ì„¤ì •
        config.multi_stage.stages = target_stages
        config.multi_stage.improvement_threshold = -1.0  # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰
        
        # ì†ŒìŠ¤ ëª¨ë¸ ì •ë³´ ì €ì¥ (ë©”íƒ€ë°ì´í„°ë¡œ)
        config.source_model_path = source_model_path
        
        return config
    
    def create_fine_tuning_stages(self, 
                                base_lr: float = 1e-5,
                                num_epochs: int = 5) -> List[TrainingStageConfig]:
        """ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ ìƒì„±"""
        return [
            # ë‹¨ê³„ 1: ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì • (ë‚®ì€ í•™ìŠµë¥ )
            TrainingStageConfig(
                name="full_fine_tuning",
                description="ì „ì²´ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •",
                num_epochs=num_epochs,
                batch_size=16,
                learning_rate=base_lr,
                enable_augmentation=True,
                augmentation_strength=0.5,
                dropout_rate=0.1,
                label_smoothing=0.05
            ),
            
            # ë‹¨ê³„ 2: ë¶„ë¥˜ê¸°ë§Œ ë¯¸ì„¸ ì¡°ì • (ë†’ì€ í•™ìŠµë¥ )
            TrainingStageConfig(
                name="classifier_fine_tuning",
                description="ë¶„ë¥˜ê¸° ë ˆì´ì–´ë§Œ ë¯¸ì„¸ ì¡°ì •",
                num_epochs=num_epochs,
                batch_size=16,
                learning_rate=base_lr * 5,
                enable_augmentation=True,
                augmentation_strength=0.7,
                dropout_rate=0.15,
                label_smoothing=0.1,
                freeze_encoder=True  # ì¸ì½”ë” ë™ê²°
            )
        ]
    
    def create_domain_adaptation_stages(self, 
                                      base_lr: float = 5e-6) -> List[TrainingStageConfig]:
        """ë„ë©”ì¸ ì ì‘ ë‹¨ê³„ ìƒì„±"""
        return [
            # ë‹¨ê³„ 1: ì ì§„ì  í•´ì œ (Gradual Unfreezing)
            TrainingStageConfig(
                name="gradual_unfreezing",
                description="ì ì§„ì  ë ˆì´ì–´ í•´ì œ",
                num_epochs=3,
                batch_size=12,
                learning_rate=base_lr,
                enable_augmentation=True,
                augmentation_strength=1.2,
                dropout_rate=0.2,
                label_smoothing=0.15
            ),
            
            # ë‹¨ê³„ 2: ì „ì²´ ëª¨ë¸ ì ì‘
            TrainingStageConfig(
                name="full_adaptation", 
                description="ì „ì²´ ëª¨ë¸ ë„ë©”ì¸ ì ì‘",
                num_epochs=5,
                batch_size=8,
                learning_rate=base_lr * 2,
                enable_augmentation=True,
                augmentation_strength=0.8,
                dropout_rate=0.1,
                label_smoothing=0.05
            )
        ]
    
    def run_transfer_learning(self, 
                            source_model_path: str,
                            transfer_type: str = "fine_tuning",
                            **kwargs) -> Dict:
        """ì „ì´ í•™ìŠµ ì‹¤í–‰"""
        logger.info(f"ğŸ”„ ì „ì´ í•™ìŠµ ì‹œì‘: {transfer_type}")
        logger.info(f"ì†ŒìŠ¤ ëª¨ë¸: {source_model_path}")
        
        # ì „ì´ í•™ìŠµ ë‹¨ê³„ ìƒì„±
        if transfer_type == "fine_tuning":
            stages = self.create_fine_tuning_stages(**kwargs)
        elif transfer_type == "domain_adaptation":
            stages = self.create_domain_adaptation_stages(**kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì „ì´ í•™ìŠµ íƒ€ì…: {transfer_type}")
        
        # ì„¤ì • ìƒì„±
        config = self.create_transfer_config(
            source_model_path=source_model_path,
            target_stages=stages,
            experiment_suffix=transfer_type
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = AdvancedSignLanguageTrainer(config)
        
        # ì†ŒìŠ¤ ëª¨ë¸ ë¡œë“œ (í•„ìš”ì‹œ)
        # TODO: ëª¨ë¸ ê°€ì¤‘ì¹˜ ì‚¬ì „ ë¡œë“œ ê¸°ëŠ¥ êµ¬í˜„
        
        # ì „ì´ í•™ìŠµ ì‹¤í–‰
        results = trainer.train_multi_stage()
        
        logger.info("âœ… ì „ì´ í•™ìŠµ ì™„ë£Œ")
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜ - ëª¨ë¸ ë¹„êµ ë°ëª¨"""
    logging.basicConfig(level=logging.INFO)
    
    # ëª¨ë¸ ë¹„êµ
    comparator = ModelComparator()
    df = comparator.compare_models()
    comparator.print_comparison(df)
    
    if len(df) > 1:
        comparator.plot_comparison(df, "model_comparison.png")
    
    # ì§€ì†ì  í•™ìŠµ ì˜ˆì œ (ì£¼ì„ ì²˜ë¦¬)
    # base_config = AdvancedTrainingConfig()
    # manager = ContinualLearningManager(base_config)
    
    # if len(df) > 0:
    #     best_model_path = df.iloc[0]['path']
    #     results = manager.run_transfer_learning(
    #         source_model_path=best_model_path,
    #         transfer_type="fine_tuning",
    #         base_lr=1e-5,
    #         num_epochs=2
    #     )
    #     print("ì „ì´ í•™ìŠµ ê²°ê³¼:", results)

if __name__ == "__main__":
    main()
