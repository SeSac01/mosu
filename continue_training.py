#!/usr/bin/env python3
"""
ì €ì¥ëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
"""

import argparse
import logging
import torch
from pathlib import Path
from typing import Dict, Optional

from advanced_config import AdvancedTrainingConfig, TrainingStageConfig
from advanced_trainer import AdvancedSignLanguageTrainer
from sign_language_model import SequenceToSequenceSignModel

class ModelContinuationTrainer:
    """ì €ì¥ëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def list_available_models(self, checkpoint_dir: str = "./advanced_checkpoints") -> Dict:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì €ì¥ëœ ëª¨ë¸ë“¤ì„ ë‚˜ì—´"""
        checkpoint_path = Path(checkpoint_dir)
        
        if not checkpoint_path.exists():
            self.logger.warning(f"ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {checkpoint_dir}")
            return {}
        
        models = {}
        
        # ë‹¨ê³„ë³„ ëª¨ë¸ë“¤ ì°¾ê¸° (stage_*.pt)
        stage_models = list(checkpoint_path.glob("stage_*.pt"))
        for model_file in sorted(stage_models):
            try:
                checkpoint = torch.load(model_file, map_location='cpu')
                models[model_file.stem] = {
                    'path': model_file,
                    'stage_idx': checkpoint.get('stage_idx', 0),
                    'stage_name': checkpoint.get('stage_name', 'unknown'),
                    'description': checkpoint.get('stage_description', ''),
                    'val_loss': checkpoint.get('val_loss', float('inf')),
                    'val_accuracy': checkpoint.get('val_accuracy', 0.0),
                    'training_time': checkpoint.get('training_time', 0.0)
                }
            except Exception as e:
                self.logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_file} - {e}")
        
        # best_model íŒŒì¼ë“¤ë„ ì°¾ê¸° (stage_*.ptê°€ ì—†ëŠ” ê²½ìš° ëŒ€ì•ˆìœ¼ë¡œ)
        if not models:
            best_models = list(checkpoint_path.glob("best_model_stage_*.pt"))
            for model_file in sorted(best_models):
                try:
                    checkpoint = torch.load(model_file, map_location='cpu')
                    stage_num = model_file.stem.split('_')[-1]  # stage_Xì—ì„œ X ì¶”ì¶œ
                    models[model_file.stem] = {
                        'path': model_file,
                        'stage_idx': int(stage_num) if stage_num.isdigit() else 0,
                        'stage_name': checkpoint.get('stage', 'unknown'),
                        'description': f"Best model from stage {stage_num}",
                        'val_loss': checkpoint.get('val_loss', float('inf')),
                        'val_accuracy': checkpoint.get('val_accuracy', 0.0),
                        'training_time': 0.0
                    }
                except Exception as e:
                    self.logger.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_file} - {e}")
        
        return models
    
    def load_model_for_continuation(self, model_path: str, new_config: AdvancedTrainingConfig) -> SequenceToSequenceSignModel:
        """ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì¶”ê°€ í•™ìŠµ ì¤€ë¹„"""
        checkpoint = torch.load(model_path, map_location='cpu')
        
        # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        stage_config = checkpoint.get('stage_config', {})
        
        # ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± (ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ì…‹ì—ì„œ vocab_sizeë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
        model = SequenceToSequenceSignModel(
            vocab_size=442,  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ë°ì´í„°ì…‹ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
            embed_dim=256,
            num_encoder_layers=6,
            num_decoder_layers=4,
            num_heads=8,
            dim_feedforward=1024,
            dropout=stage_config.get('dropout_rate', 0.1),
            max_seq_len=500,
            label_smoothing=stage_config.get('label_smoothing', 0.0)
        )
        
        # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        self.logger.info(f"  Stage: {checkpoint.get('stage_name', 'unknown')}")
        self.logger.info(f"  Val Loss: {checkpoint.get('val_loss', 0):.4f}")
        self.logger.info(f"  Val Acc: {checkpoint.get('val_accuracy', 0):.3f}")
        
        return model
    
    def create_continuation_config(self, base_model_info: Dict, continuation_type: str = "fine_tune") -> AdvancedTrainingConfig:
        """ì¶”ê°€ í•™ìŠµì„ ìœ„í•œ ì„¤ì • ìƒì„±"""
        config = AdvancedTrainingConfig()
        
        # ì‹¤í—˜ëª… ì„¤ì •
        config.experiment_name = f"continuation_{base_model_info['stage_name']}_{continuation_type}"
        
        # ì¶”ê°€ í•™ìŠµ ìœ í˜•ë³„ ì„¤ì •
        if continuation_type == "fine_tune":
            # ë¯¸ì„¸ ì¡°ì •
            stages = [
                TrainingStageConfig(
                    name="fine_tune_continuation",
                    description=f"Stage {base_model_info['stage_idx']} ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •",
                    num_epochs=10,
                    batch_size=16,
                    learning_rate=5e-6,  # ë§¤ìš° ì‘ì€ í•™ìŠµë¥ 
                    enable_augmentation=True,
                    augmentation_strength=0.3,
                    dropout_rate=0.05,
                    label_smoothing=0.05
                )
            ]
        elif continuation_type == "regularization":
            # ì •ê·œí™” ê°•í™”
            stages = [
                TrainingStageConfig(
                    name="regularization_continuation",
                    description=f"Stage {base_model_info['stage_idx']} ëª¨ë¸ ì •ê·œí™” ê°•í™”",
                    num_epochs=15,
                    batch_size=24,
                    learning_rate=2e-5,
                    enable_augmentation=True,
                    augmentation_strength=1.2,
                    dropout_rate=0.25,
                    label_smoothing=0.15
                )
            ]
        elif continuation_type == "exploration":
            # íƒìƒ‰ì  í•™ìŠµ
            stages = [
                TrainingStageConfig(
                    name="exploration_continuation",
                    description=f"Stage {base_model_info['stage_idx']} ëª¨ë¸ íƒìƒ‰ì  í•™ìŠµ",
                    num_epochs=20,
                    batch_size=32,
                    learning_rate=1e-4,
                    enable_augmentation=True,
                    augmentation_strength=1.0,
                    dropout_rate=0.15,
                    label_smoothing=0.1
                )
            ]
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” continuation_type: {continuation_type}")
        
        config.multi_stage.stages = stages
        config.multi_stage.improvement_threshold = -1.0  # ëª¨ë“  ë‹¨ê³„ ì§„í–‰
        
        return config
    
    def continue_training(self, model_name: str, continuation_type: str = "fine_tune", 
                         checkpoint_dir: str = "./advanced_checkpoints") -> Dict:
        """ì €ì¥ëœ ëª¨ë¸ë¡œë¶€í„° ì¶”ê°€ í•™ìŠµ ìˆ˜í–‰"""
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
        available_models = self.list_available_models(checkpoint_dir)
        
        if model_name not in available_models:
            raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_name}")
        
        model_info = available_models[model_name]
        
        # ëª¨ë¸ ë¡œë“œ
        config = self.create_continuation_config(model_info, continuation_type)
        model = self.load_model_for_continuation(str(model_info['path']), config)
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í•™ìŠµ
        trainer = AdvancedSignLanguageTrainer(config)
        
        # ëª¨ë¸ì„ íŠ¸ë ˆì´ë„ˆì— ì„¤ì • (ì¶”ê°€ í•™ìŠµì„ ìœ„í•´)
        trainer.model = model
        
        # ì¶”ê°€ í•™ìŠµ ì‹¤í–‰
        results = trainer.train_multi_stage()
        
        return results

def main():
    parser = argparse.ArgumentParser(description="ì €ì¥ëœ ëª¨ë¸ ê¸°ë°˜ ì¶”ê°€ í•™ìŠµ")
    parser.add_argument("--list", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥")
    parser.add_argument("--model", type=str, help="ì¶”ê°€ í•™ìŠµí•  ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--type", choices=["fine_tune", "regularization", "exploration"],
                       default="fine_tune", help="ì¶”ê°€ í•™ìŠµ ìœ í˜•")
    parser.add_argument("--checkpoint-dir", type=str, default="./advanced_checkpoints",
                       help="ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
    )
    
    trainer = ModelContinuationTrainer()
    
    if args.list:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¶œë ¥
        models = trainer.list_available_models(args.checkpoint_dir)
        
        if not models:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
        print("="*80)
        for model_name, info in models.items():
            print(f"ğŸ”¹ {model_name}")
            print(f"   Stage: {info['stage_name']} ({info['description']})")
            print(f"   Performance: Val Loss {info['val_loss']:.4f}, Val Acc {info['val_accuracy']:.3f}")
            print(f"   Training Time: {info['training_time']:.1f}ì´ˆ")
            print()
    
    elif args.model:
        # ì¶”ê°€ í•™ìŠµ ìˆ˜í–‰
        try:
            results = trainer.continue_training(args.model, args.type, args.checkpoint_dir)
            print(f"âœ… '{args.model}' ëª¨ë¸ì˜ ì¶”ê°€ í•™ìŠµ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì¶”ê°€ í•™ìŠµ ì‹¤íŒ¨: {e}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
